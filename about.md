---
title: About
layout: page
---
![Profile Image]({% if site.external-image %}{{ site.picture }}{% else %}{{ site.url }}/{{ site.picture }}{% endif %})

<p>I'm an <a href="https://www.microsoft.com/en-us/research/people/edwardhu/">AI researcher at Microsoft</a> working on the large-scale deployment of GPT-3, principled approaches to large model training, and theories of infinitely wide neural networks.</p>

<p>I spent a year as an <a href="https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/">AI resident</a> at Microsoft Research, after graduating with a Bachelor of Science in Computer Science and Cognitive Science from Johns Hopkins University. While at JHU, I worked at the <a href="https://www.clsp.jhu.edu/">Center for Language and Speech Processing</a> and was advised by <a href="https://www.cs.jhu.edu/~vandurme/">Ben Van Durme</a>.</p>

<h2>Some Publications</h2>

<ul class="publications">
	LoRA: Low-Rank Adaptation of Large Language Models<br>
	<i>Pre-print</i><br>
	[<a href="https://arxiv.org/abs/2106.09685">Paper</a> | <a href="https://github.com/microsoft/LoRA">Code</a>]<br>
	Feature Learning in Infinite-Width Neural Networks<br>
	<i>ICML 2021</i><br>
	[<a href="https://arxiv.org/abs/2011.14522">Paper</a> | <a href="https://www.microsoft.com/en-us/research/blog/on-infinitely-wide-neural-networks-that-exhibit-feature-learning/">Blog</a> | <a href="https://github.com/edwardjhu/TP4">Code</a>]<br>
	Improved Image Wasserstein Attacks and Defenses<br>
	<i>Best Paper at ICLR Trustworthy ML Workshop 2020</i><br>
	[<a href="https://arxiv.org/abs/2004.12478">Paper</a> | <a href="https://github.com/edwardjhu/improved_wasserstein">Code</a>]<br>
</ul>