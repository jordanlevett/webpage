---
title: About
layout: page
---
![Profile Image]({% if site.external-image %}{{ site.picture_hd }}{% else %}{{ site.url }}/{{ site.picture_hd }}{% endif %})

<p>I am pursuing my Ph.D. at Mila supervised by <a href="https://yoshuabengio.org/">Yoshua Bengio</a>. My research on robust reasoning is generously supported by a <a href="https://bourses.umontreal.ca/repertoire-des-bourses/detail-dune-bourse/bid/1812/bsl/Bourses-en-intelligence-artificielle-des-%C3%89tudes-sup%C3%A9rieures-et-postdoctorales-ESP-2022-2023/">Université de Montréal Graduate AI Fellowship</a>.</p>

<p>Before that, I was a researcher at Microsoft working on the large-scale deployment of GPT-3, principled approaches to large model training, and theories of infinitely wide neural networks.</p>

<p>I spent a wonderful year as an <a href="https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/">AI resident</a> at Microsoft Research Redmond, where I worked closely with <a href="https://www.microsoft.com/en-us/research/people/gregyang/">Greg Yang</a>. I obtained my bachelor's degree in Computer Science and Cognitive Science from Johns Hopkins University, where I was advised by <a href="https://www.cs.jhu.edu/~vandurme/">Ben Van Durme</a>.</p>

<h2>Some Publications</h2>

<ul class="publications">
	Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer<br>
	<i>NeurIPS 2021</i><br>
	[<a href="https://arxiv.org/abs/2203.03466">Paper</a> | <a href="https://www.microsoft.com/en-us/research/blog/%c2%b5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/">Blog</a> | <a href="https://github.com/microsoft/mup">Code</a> | <a href="https://www.theregister.com/2022/03/14/microsoft_openai_mutransfer/">TheRegister</a> | <a href="https://medium.com/syncedreview/microsoft-openais-%C2%B5transfer-zero-shot-hyperparameter-transfer-method-tunes-gpt-3-s-3c820a75bcad">SyncedReview</a> | <a href="https://analyticsindiamag.com/interview-with-the-team-behind-microsofts-%C2%B5transfer/">AIM</a>]<br>
	<br>
	LoRA: Low-Rank Adaptation of Large Language Models<br>
	<i>ICLR 2022</i><br>
	[<a href="https://arxiv.org/abs/2106.09685">Paper</a> | <a href="https://github.com/microsoft/LoRA">Code</a> | Mentioned in <a href="https://blogs.microsoft.com/ai-for-business/ai-at-scale-technology/#:~:text=We%20also%20developed,or%20downstream%20task.">"The innovation behind AI at Scale"</a>]<br>
	<br>
	GFlowNet Foundations<br>
	<i>JMLR 2023</i><br>
	[<a href="https://arxiv.org/abs/2111.09266">Paper</a>]<br>
	<br>
	Feature Learning in Infinite-Width Neural Networks<br>
	<i>ICML 2021</i><br>
	[<a href="https://arxiv.org/abs/2011.14522">Paper</a> | <a href="https://www.microsoft.com/en-us/research/blog/on-infinitely-wide-neural-networks-that-exhibit-feature-learning/">Blog</a> | <a href="https://github.com/edwardjhu/TP4">Code</a>]<br>
	<br>
	Improved Image Wasserstein Attacks and Defenses<br>
	<i>Best Paper at ICLR Trustworthy ML Workshop 2020</i><br>
	[<a href="https://arxiv.org/abs/2004.12478">Paper</a> | <a href="https://github.com/edwardjhu/improved_wasserstein">Code</a>]<br>
	<br>
	Randomized smoothing of all shapes and sizes<br>
	<i>ICML 2020</i><br>
	[<a href="https://arxiv.org/abs/2002.08118">Paper</a> | <a href="https://github.com/tonyduan/rs4a">Code</a>]<br>
	<br>
	Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting<br>
	<i>NAACL 2019</i><br>
	[<a href="https://aclanthology.org/N19-1090.pdf">Paper</a> | <a href="https://nlp.jhu.edu/parabank/">Dataset</a>]<br>
</ul>