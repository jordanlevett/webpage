---
title: About
layout: page
---
![Profile Image]({% if site.external-image %}{{ site.picture_hd }}{% else %}{{ site.url }}/{{ site.picture_hd }}{% endif %})

<p>I started in Jan 2022 as a PhD student at Mila, where I study deep learning under the supervision of <a href="https://yoshuabengio.org/">Yoshua Bengio</a>.</p>

<p>Before that, I was an AI researcher at Microsoft working on the large-scale deployment of GPT-3, principled approaches to the training of large deep learning model, and theories of infinitely wide neural networks. I'm broadly interested in machine learning and natural language processing.</p>

<p>I spent a year as an <a href="https://www.microsoft.com/en-us/research/academic-program/microsoft-ai-residency-program/">AI resident</a> at Microsoft Research, after graduating with a BSc in Computer Science and Cognitive Science from Johns Hopkins University. While at JHU, I worked for the <a href="https://www.clsp.jhu.edu/">Center for Language and Speech Processing</a> and was advised by <a href="https://www.cs.jhu.edu/~vandurme/">Ben Van Durme</a>.</p>

<h2>Some Publications</h2>

<ul class="publications">
	Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer<br>
	<i>NeurIPS 2021</i><br>
	[<a href="https://arxiv.org/abs/2203.03466">Paper</a> | <a href="https://www.microsoft.com/en-us/research/blog/%c2%b5transfer-a-technique-for-hyperparameter-tuning-of-enormous-neural-networks/">Blog</a> | <a href="https://github.com/microsoft/mup">Code</a> | <a href="https://www.theregister.com/2022/03/14/microsoft_openai_mutransfer/">TheRegister</a> | <a href="https://medium.com/syncedreview/microsoft-openais-%C2%B5transfer-zero-shot-hyperparameter-transfer-method-tunes-gpt-3-s-3c820a75bcad">SyncedReview</a>]<br>
	<br>
	LoRA: Low-Rank Adaptation of Large Language Models<br>
	<i>ICLR 2022</i><br>
	[<a href="https://arxiv.org/abs/2106.09685">Paper</a> | <a href="https://github.com/microsoft/LoRA">Code</a>]<br>
	<br>
	GFlowNet Foundations<br>
	<i>Pre-print</i><br>
	[<a href="https://arxiv.org/abs/2111.09266">Paper</a>]<br>
	<br>
	Feature Learning in Infinite-Width Neural Networks<br>
	<i>ICML 2021</i><br>
	[<a href="https://arxiv.org/abs/2011.14522">Paper</a> | <a href="https://www.microsoft.com/en-us/research/blog/on-infinitely-wide-neural-networks-that-exhibit-feature-learning/">Blog</a> | <a href="https://github.com/edwardjhu/TP4">Code</a>]<br>
	<br>
	Improved Image Wasserstein Attacks and Defenses<br>
	<i>Best Paper at ICLR Trustworthy ML Workshop 2020</i><br>
	[<a href="https://arxiv.org/abs/2004.12478">Paper</a> | <a href="https://github.com/edwardjhu/improved_wasserstein">Code</a>]<br>
	<br>
	Randomized smoothing of all shapes and sizes<br>
	<i>ICML 2020</i><br>
	[<a href="https://arxiv.org/abs/2002.08118">Paper</a> | <a href="https://github.com/tonyduan/rs4a">Code</a>]<br>
	<br>
	Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting<br>
	<i>NAACL 2019</i><br>
	[<a href="https://aclanthology.org/N19-1090.pdf">Paper</a> | <a href="https://nlp.jhu.edu/parabank/">Dataset</a>]<br>
</ul>